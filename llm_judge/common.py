import ast
import dataclasses
import json
import logging
import os
import re
import time
from pathlib import Path
from typing import Union, Optional

import openai
import tiktoken
from dotenv import load_dotenv

logger = logging.getLogger(__name__)

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.organization = os.getenv("OPENAI_ORGANIZATION")

# Data paths
JP_BENCH_DIR = Path(__file__).resolve().parent.parent / "data" / "jp_bench"
QUESTION_FILE = JP_BENCH_DIR / "question.jsonl"
PREDICTION_DIR = JP_BENCH_DIR / "model_answer"
REFERENCE_DIR = JP_BENCH_DIR / "reference_answer"
JUDGEMENT_DIR = JP_BENCH_DIR / "model_judgment"
JUDGEMENT_PROMPT_FILE = JP_BENCH_DIR / "judge_prompts.jsonl"

# API setting constants
API_MAX_RETRY = 16
API_RETRY_SLEEP = 10

# Categories that need reference answers
NEED_REF_CATS = ["math", "reasoning", "coding"]

# Extract scores from judgments
two_score_pattern = re.compile(r"\[\[(\d+\.?\d*),\s?(\d+\.?\d*)]]")
two_score_pattern_backup = re.compile(r"\[(\d+\.?\d*),\s?(\d+\.?\d*)]")
one_score_pattern = re.compile(r"\[\[(\d+\.?\d*)]]")
one_score_pattern_another_format = re.compile(r"\[\[rating:(\d+)]]")
one_score_pattern_another_format2 = re.compile(r"\[\[rating: (\d+)]]")


@dataclasses.dataclass
class Judge:
    model: str
    prompt_template: dict

    def judge(self, **kwargs):
        messages = [
            {"role": "system", "content": self.prompt_template["system_prompt"]},
            {
                "role": "user",
                "content": self.prompt_template["prompt_template"].format(**kwargs),
            },
        ]
        for _ in range(API_MAX_RETRY):
            try:
                response = openai.ChatCompletion.create(
                    model=self.model,
                    messages=messages,
                    temperature=0,
                    max_tokens=2048,
                )
                return response["choices"][0]["message"]["content"]
            except openai.error.OpenAIError as e:
                logger.warning(f"OpenAI API error: {e}")
                time.sleep(API_RETRY_SLEEP)


@dataclasses.dataclass
class MatchSingle:
    question: dict
    model: str
    answer: dict
    judge: Judge
    ref_answer: Optional[dict] = None

    def __post_init__(self) -> None:
        if self.judge.prompt_template["type"] != "single":
            raise ValueError(
                f"invalid judge type: {self.judge.prompt_template['type']}"
            )
        if self.judge.prompt_template["output_format"] != "[[rating]]":
            raise ValueError(
                f"Invalid output format: {self.judge.prompt_template['output_format']}"
            )

    def play(self):
        """Play a single match."""
        kwargs = {
            "question": self.question["turns"][0],
            "answer": self.answer["choices"][0]["turns"][0],
        }
        if self.ref_answer:
            kwargs["ref_answer_1"] = self.ref_answer["choices"][0]["turns"][0]
        judgment = self.judge.judge(**kwargs)
        match = (
            re.search(one_score_pattern, judgment)
            or re.search(one_score_pattern_another_format, judgment)
            or re.search(one_score_pattern_another_format2, judgment)
        )
        if match:
            score = ast.literal_eval(match.groups()[0])
        else:
            score = -1
        return {
            "model": self.model,
            "question_id": self.question["question_id"],
            "question": self.question["turns"][0],
            "answer": self.answer["choices"][0]["turns"][0],
            "judgment": judgment,
            "score": score,
            "judge_model": self.judge.model,
            "judge_prompt": self.judge.prompt_template["name"],
            "tstamp": time.time(),
        }

    def estimate_cost(self) -> float:
        enc = tiktoken.encoding_for_model(self.judge.model)
        num_input_tokens = (
            len(enc.encode(self.question["turns"][0]))
            + len(enc.encode(self.answer["choices"][0]["turns"][0]))
            + len(enc.encode(self.judge.prompt_template["system_prompt"]))
            + len(enc.encode(self.judge.prompt_template["prompt_template"]))
        )
        if self.ref_answer:
            num_input_tokens += len(
                enc.encode(self.ref_answer["choices"][0]["turns"][0])
            )
        num_output_tokens = 200  # Estimated from a few samples
        if self.judge.model == "gpt-4":
            return 0.03 * num_input_tokens + 0.06 * num_output_tokens / 1_000
        elif self.judge.model == "gpt-3.5-turbo":
            return 0.001 * num_input_tokens + 0.002 * num_output_tokens / 1_000
        raise AssertionError


@dataclasses.dataclass
class MatchPair:
    question: dict
    model_1: str
    model_2: str
    answer_1: dict
    answer_2: dict
    judge: Judge
    ref_answer: Optional[dict] = None

    def __post_init__(self) -> None:
        if self.judge.prompt_template["type"] != "pairwise":
            raise ValueError(
                f"invalid judge type: {self.judge.prompt_template['type']}"
            )
        if self.judge.prompt_template["output_format"] != "[[A]]":
            raise ValueError(
                f"Invalid output format: {self.judge.prompt_template['output_format']}"
            )

    def play(self):
        """Play a pairwise match."""

        def play(answer_a, answer_b):
            kwargs = {
                "question": self.question["turns"][0],
                "answer_a": answer_a["choices"][0]["turns"][0],
                "answer_b": answer_b["choices"][0]["turns"][0],
            }
            if self.ref_answer is not None:
                kwargs["ref_answer_1"] = self.ref_answer["choices"][0]["turns"][0]
            judgment = self.judge.judge(**kwargs)

            if "[[A]]" in judgment:
                winner = "A"
            elif "[[B]]" in judgment:
                winner = "B"
            elif "[[C]]" in judgment:
                winner = "tie"
            else:
                winner = "error"

            return winner, judgment

        g1_winner, g1_judgment = play(self.answer_1, self.answer_2)
        g1_winner = "model_1" if g1_winner == "A" else "model_2"

        g2_winner, g2_judgment = play(self.answer_2, self.answer_1)
        g2_winner = "model_2" if g2_winner == "A" else "model_1"

        result = {
            "model_1": self.model_1,
            "model_2": self.model_2,
            "question_id": self.question["question_id"],
            "question": self.question["turns"][0],
            "answer_1": self.answer_1["choices"][0]["turns"][0],
            "answer_2": self.answer_2["choices"][0]["turns"][0],
            "g1_judgment": g1_judgment,
            "g2_judgment": g2_judgment,
            "g1_winner": g1_winner,
            "g2_winner": g2_winner,
            "judge_model": self.judge.model,
            "judge_prompt": self.judge.prompt_template["name"],
            "tstamp": time.time(),
        }
        return result

    def estimate_cost(self) -> float:
        enc = tiktoken.encoding_for_model(self.judge.model)
        num_input_tokens = (
            len(enc.encode(self.question["turns"][0]))
            + len(enc.encode(self.answer_1["choices"][0]["turns"][0]))
            + len(enc.encode(self.answer_2["choices"][0]["turns"][0]))
            + len(enc.encode(self.judge.prompt_template["system_prompt"]))
            + len(enc.encode(self.judge.prompt_template["prompt_template"]))
        )
        if self.ref_answer:
            num_input_tokens += len(
                enc.encode(self.ref_answer["choices"][0]["turns"][0])
            )
        num_output_tokens = 200  # Estimated from a few samples
        if self.judge.model == "gpt-4":
            return 2 * (0.03 * num_input_tokens + 0.06 * num_output_tokens) / 1_000
        elif self.judge.model == "gpt-3.5-turbo":
            return 2 * (0.001 * num_input_tokens + 0.002 * num_output_tokens) / 1_000
        raise AssertionError


def load_questions(question_file: Union[str, Path]) -> list[dict]:
    """Load questions from a file.

    Args:
        question_file (Union[str, Path]): The question file.
    """
    with open(question_file, "r") as fin:
        return [json.loads(line) for line in fin]


def get_model_list(answer_dir: Union[str, Path]):
    """Get model list from answer directory.

    Args:
        answer_dir (Union[str, Path]): The answer directory.
    """
    return [path.name for path in Path(answer_dir).iterdir()]


def load_model_answers(answer_dir: Union[str, Path]):
    """Load model answers.

    Args:
        answer_dir (Union[str, Path]): The answer directory.
    """
    answers = {}
    with open(Path(answer_dir) / "results.jsonl", "r") as fin:
        for line in fin:
            answer = json.loads(line)
            answers[answer["question_id"]] = answer
    return answers


def load_judgements(judgement_dir: Union[str, Path]):
    """Load judgements.

    Args:
        judgement_dir (Union[str, Path]): The judgement directory.
    """
    judgements = {}
    for path in Path(judgement_dir).glob("*.jsonl"):
        with open(path, "r") as fin:
            results = []
            for line in fin:
                results.append(json.loads(line))
            judgements[path.stem] = results
    return judgements


def load_judge_prompts(prompt_file: Union[str, Path]):
    """Load judge prompts.

    Args:
        prompt_file (Union[str, Path]): The prompt file.
    """
    prompts = {}
    with open(prompt_file) as fin:
        for line in fin:
            line = json.loads(line)
            prompts[line["name"]] = line
    return prompts


def filter_single_judgements(
    result_id_results_map: dict[str, list[dict]], model_list: Optional[list[str]] = None
):
    """Filter results by specified models.

    Args:
        result_id_results_map (dict[str, list[dict]]): A dict of results.
        model_list (list[str], optional): A list of models. Defaults to None.
    """
    if model_list is None:
        return result_id_results_map
    filtered_result_id_results_map = {}
    for result_id, results in result_id_results_map.items():
        result = results[0]
        if result["model"] in model_list:
            filtered_result_id_results_map[result_id] = results
    return filtered_result_id_results_map


def filter_pairwise_judgements(
    result_id_results_map: dict[str, list[dict]],
    model_list: Optional[list[str]] = None,
    baseline_model: Optional[str] = None,
):
    """Filter results by specified models.

    Args:
        result_id_results_map (dict[str, list[dict]]): A dict of results.
        model_list (list[str], optional): A list of models. Defaults to None.
        baseline_model (str, optional): The baseline model. Defaults to None.
    """
    filtered_result_id_results_map = {}
    for result_id, results in result_id_results_map.items():
        result = results[0]
        if model_list and baseline_model:
            if (
                result["model_1"] in model_list and result["model_2"] == baseline_model
            ) or (
                result["model_2"] in model_list and result["model_1"] == baseline_model
            ):
                filtered_result_id_results_map[result_id] = results
        elif model_list and baseline_model is None:
            if result["model_1"] in model_list and result["model_2"] in model_list:
                filtered_result_id_results_map[result_id] = results
        elif model_list is None and baseline_model:
            if (
                result["model_1"] == baseline_model
                or result["model_2"] == baseline_model
            ):
                filtered_result_id_results_map[result_id] = results
        else:
            filtered_result_id_results_map[result_id] = results
    return filtered_result_id_results_map
